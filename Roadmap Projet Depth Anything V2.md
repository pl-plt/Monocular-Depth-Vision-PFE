# **ROADMAP: Projet Depth Anything V2**

## **Lien discussion :** 

https://claude.ai/share/45eb5773-4d5c-49ef-b4c6-ab507072fe7f

## **PFE \- Recr√©ation d'un algorithme de vision par ordinateur**

**Contexte :** Reproduction partielle de Depth Anything V2 avec une carte NVIDIA H100  
 **Dur√©e totale :** 24 semaines (6 mois)  
 **Pr√©requis :** Connaissances Python, PyTorch, bases ML/Deep Learning  
 **Objectif :** Entra√Æner un mod√®le ViT-Small fonctionnel sur un subset r√©aliste de donn√©es

---

## **PHASE 0 : Baseline et Validation Setup (Semaine 1\)**

### **Objectifs**

Prouver que votre infrastructure fonctionne et √©tablir une r√©f√©rence de performance claire.

### **T√¢ches concr√®tes**

1. **Setup environnement H100**

   * Installer PyTorch 2.x avec CUDA 12+  
   * V√©rifier disponibilit√© GPU : `nvidia-smi`, test torch.cuda.is\_available()  
   * Installer d√©pendances : `transformers`, `timm`, `opencv-python`, `pillow`  
2. **T√©l√©charger les poids officiels**

   * R√©cup√©rer Depth-Anything-V2-Small depuis le repo GitHub officiel  
   * Charger le mod√®le pr√©-entra√Æn√© en m√©moire  
   * V√©rifier la taille : \~100 MB, 25M param√®tres  
3. **Tester l'inf√©rence**

   * T√©l√©charger 50-100 images de test (NYU-Depth V2 ou KITTI)  
   * Ex√©cuter la pr√©diction sur ces images  
   * Visualiser les depth maps g√©n√©r√©es (colormap viridis)  
   * Mesurer le temps d'inf√©rence moyen par image  
4. **Calculer les m√©triques de r√©f√©rence**

   * Sur un subset de NYU-D test set (654 images)  
   * M√©triques : AbsRel, RMSE, Œ¥1, Œ¥2, Œ¥3  
   * **Exemple target :** AbsRel \< 0.05, Œ¥1 \> 0.99 (valeurs du papier)

### **Livrables**

* ‚úÖ Script Python fonctionnel d'inf√©rence  
* ‚úÖ Notebook avec visualisations (images \+ depth maps)  
* ‚úÖ Tableau de m√©triques baseline (vos r√©sultats vs papier)  
* ‚úÖ Confirmation acc√®s H100 \+ temps GPU disponible/semaine

### **Crit√®res de succ√®s**

* Mod√®le officiel tourne sur votre H100 sans erreur  
* Temps inf√©rence \< 0.5 sec/image  
* M√©triques √† ¬±5% des valeurs publi√©es

### **Plan B si blocage**

* Si GPU inaccessible : travailler en local avec CPU sur 10 images (lent mais validable)  
* Si poids introuvables : utiliser MiDaS v3 comme baseline alternative

---

## **PHASE 1 : Cadrage Th√©orique et Architecture (Semaines 2-3)**

### **Objectifs**

Ma√Ætriser les fondamentaux math√©matiques et techniques avant de coder.

### **T√¢ches concr√®tes**

**Semaine 2 : √âtude du papier**

1. **Lecture approfondie**

   * Section sur l'architecture (ViT encoder \+ DPT decoder)  
   * Focus sur les 3 contributions cl√©s :  
     * Entra√Ænement Teacher sur donn√©es synth√©tiques uniquement  
     * Teacher DINOv2-Giant (1.1B params) pour pseudo-labels  
     * Student distillation sur images r√©elles non-√©tiquet√©es  
2. **Comprendre les loss functions**

   * Scale-invariant loss : $\\mathcal{L}\_{ssi} \= \\sqrt{\\frac{1}{n}\\sum(d\_i \- d\_i^*)^2 \- \\frac{\\lambda}{n^2}(\\sum(d\_i \- d\_i^*))^2}$  
   * Gradient matching loss : $\\mathcal{L}\_{gm} \= \\frac{1}{n}\\sum||\\nabla d\_i \- \\nabla d\_i^\*||\_1$  
   * Strat√©gie top-10% loss masking (ignorer pixels avec erreurs extr√™mes)  
3. **Sch√©matiser le pipeline**

   * Dessiner le flow : Image ‚Üí DINOv2 features ‚Üí Decoder ‚Üí Depth map  
   * Identifier les poids fig√©s vs entra√Ænables

**Semaine 3 : Exploration DINOv2 et ViT**

**Tester DINOv2 pr√©-entra√Æn√©**

 from transformers import AutoModel  
dinov2 \= AutoModel.from\_pretrained("facebook/dinov2-giant")  
\# Extraire features sur une image test

from transformers import AutoModel

   dinov2 \= AutoModel.from\_pretrained("facebook/dinov2-giant")

   \# Extraire features sur une image test

1. **Comprendre l'architecture ViT**

   * Patch embedding (16x16 patches)  
   * Multi-head attention layers  
   * Output : features multi-√©chelle (4 niveaux)  
2. **√âtudier le decoder DPT**

   * Comment fusionner les features multi-√©chelle  
   * Upsampling progressif vers r√©solution finale

### **Livrables**

* ‚úÖ Document de synth√®se (5-10 pages) : architecture, losses, data strategy  
* ‚úÖ Sch√©ma annot√© du pipeline complet (draw.io ou PowerPoint)  
* ‚úÖ Script test extraction features DINOv2 sur 10 images

### **Crit√®res de succ√®s**

* Vous pouvez expliquer le pipeline √† votre encadrant sans notes  
* Features DINOv2 extraites avec shape correcte (e.g., \[B, 1536, H/14, W/14\])

---

## **PHASE 2 : Data Engineering (Semaines 4-9) \- 6 SEMAINES**

### **Objectifs**

Constituer et pr√©parer vos datasets d'entra√Ænement avec une approche pragmatique.

### **Semaine 4-5 : S√©lection et t√©l√©chargement**

**Donn√©es synth√©tiques (pour validation, pas training)**

* Dataset : **Hypersim** (indoor synthetic)  
* Volume : 50,000 images (vs 595k original)  
* S√©lection : √©chantillonnage stratifi√© (diversit√© sc√®nes)  
* Stockage n√©cessaire : \~50 GB

**Donn√©es r√©elles non-√©tiquet√©es**

* Dataset : **SA-1B** (subset Segment Anything) ou **LSUN**  
* Volume cible progressif :  
  * Phase 1 : 50,000 images (proto rapide)  
  * Phase 2 : 200,000 images (si Phase 1 OK)  
  * Phase 3 : 500,000 images (si temps disponible)  
* Crit√®re s√©lection : r√©solution minimale 512x512, diversit√© indoor/outdoor  
* Stockage : \~200-500 GB

**Script de t√©l√©chargement**

\# Exemple structure  
datasets/  
‚îú‚îÄ‚îÄ synthetic/  
‚îÇ   ‚îú‚îÄ‚îÄ hypersim/  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ images/  
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ depth/  
‚îî‚îÄ‚îÄ real\_unlabeled/  
    ‚îî‚îÄ‚îÄ sa1b/  
        ‚îî‚îÄ‚îÄ images/

### **Semaine 6-7 : Preprocessing et dataloaders**

1. **Nettoyage des donn√©es**

   * Supprimer images corrompues (try/except PIL.Image.open)  
   * Filtrer images trop petites (\< 512px)  
   * V√©rifier coh√©rence depth maps synth√©tiques

**Pipeline de preprocessing**

 transforms \= Compose(\[  
    Resize((518, 518)),  
    RandomHorizontalFlip(p=0.5),  
    RandomCrop((480, 480)),  
    ColorJitter(brightness=0.1, contrast=0.1),  
    ToTensor(),  
    Normalize(mean=\[0.485, 0.456, 0.406\],   
             std=\[0.229, 0.224, 0.225\])  
\])

2.   
3. **Dataloader optimis√©**

   * `num_workers=8` (ajuster selon CPU)  
   * `prefetch_factor=2`  
   * `pin_memory=True` pour GPU  
   * Tester vitesse chargement : target \> 500 images/sec

### **Semaine 8-9 : Validation et versioning**

1. **Cr√©er splits train/val**

   * Train : 90% des donn√©es  
   * Val : 10% (pour monitoring overfitting)  
2. **Data versioning**

   * Utiliser DVC ou simple fichier `data_manifest.json`  
   * Documenter : source, date t√©l√©chargement, preprocessing appliqu√©  
3. **Sanity checks**

   * Visualiser 50 exemples al√©atoires  
   * V√©rifier distribution tailles, ratios aspect  
   * Plot histogramme valeurs pixels

### **Livrables**

* ‚úÖ Dataset 50k synth√©tiques t√©l√©charg√© et v√©rifi√©  
* ‚úÖ Dataset 50k r√©elles (minimum), id√©alement 200k  
* ‚úÖ Scripts preprocessing \+ dataloaders PyTorch  
* ‚úÖ Documentation data (README avec stats, exemples)  
* ‚úÖ Benchmark vitesse chargement

### **Crit√®res de succ√®s**

* Dataloader charge 500+ images/sec sur H100  
* Aucune image corrompue dans les datasets  
* Splits train/val bien s√©par√©s

### **Plan B si blocage**

* Si t√©l√©chargement trop long : utiliser ImageNet-1k (d√©j√† disponible)  
* Si stockage insuffisant : r√©duire √† 30k synth√©tiques \+ 100k r√©elles

---

## **PHASE 3 : Impl√©mentation Architecture (Semaines 10-14) \- 5 SEMAINES**

### **Objectifs**

Impl√©menter et valider l'architecture Teacher-Student avant l'entra√Ænement full-scale.

### **Semaine 10-11 : Mod√®le Teacher (DINOv2)**

**Charger DINOv2-Giant**

 teacher \= torch.hub.load('facebookresearch/dinov2', 'dinov2\_vitg14')  
teacher.eval()  \# Mode inference uniquement  
teacher.requires\_grad\_(False)  \# Figer tous les poids

1.   
2. **Ajouter le decoder head**

   * Option 1 : R√©utiliser le code officiel Depth Anything V2  
   * Option 2 : Impl√©menter DPT decoder (plus long)  
   * **Recommandation : Option 1**  
3. **Tester forward pass Teacher**

   * Input : batch \[4, 3, 518, 518\]  
   * Output attendu : \[4, 1, 518, 518\] (depth maps)  
   * V√©rifier shapes √† chaque couche

### **Semaine 12-13 : Mod√®le Student (ViT-Small)**

**Initialiser ViT-Small backbone**

 from timm import create\_model  
student\_backbone \= create\_model('vit\_small\_patch14\_dinov2',   
                                pretrained=True)

1.   
2. **Ajouter decoder identique au Teacher**

   * R√©utiliser exactement la m√™me architecture de decoder  
   * Initialiser poids al√©atoirement (sauf backbone pr√©-entra√Æn√©)

**Impl√©menter les loss functions**

 def scale\_invariant\_loss(pred, target, lambda\_=0.5):  
    \# Impl√©menter formule du papier  
    \# \+ masque top-10% erreurs  
    pass

def gradient\_matching\_loss(pred, target):  
    \# Sobel filters \+ L1 distance  
    pass

3. 

### **Semaine 14 : Validation sur toy data**

1. **Overfitting test (sanity check critique)**

   * Prendre 10 images \+ depth maps synth√©tiques  
   * Entra√Æner Student pour overfitter parfaitement  
   * **Target :** Loss \< 0.01 apr√®s 100 epochs  
   * Si √ßa ne marche pas ‚Üí bug dans loss ou architecture  
2. **Validation gradient flow**

   * V√©rifier gradients propagent jusqu'au backbone  
   * Pas de NaN ou explosion de gradients  
   * Utiliser `torch.autograd.grad_check`  
3. **Benchmark vitesse forward/backward**

   * Mesurer temps par batch (batch size 8, 16, 32\)  
   * Target : \< 0.5 sec/batch pour batch\_size=16

### **Livrables**

* ‚úÖ Mod√®le Teacher fonctionnel (inference uniquement)  
* ‚úÖ Mod√®le Student complet avec decoder  
* ‚úÖ Fonctions de loss impl√©ment√©es et test√©es  
* ‚úÖ Preuve d'overfitting sur toy dataset (loss curve)  
* ‚úÖ Code repository GitHub propre avec README

### **Crit√®res de succ√®s**

* Overfitting test r√©ussi (loss \< 0.01 sur 10 images)  
* Shapes correctes √† chaque √©tape du forward pass  
* Pas d'erreurs CUDA out-of-memory avec batch\_size=16

### **Plan B si blocage**

* Si impl√©mentation DPT trop complexe : utiliser simple CNN decoder (moins performant mais fonctionne)  
* Si probl√®mes m√©moire : r√©duire r√©solution √† 384x384

---

## **PHASE 4 : Distillation et Entra√Ænement (Semaines 15-24) \- 10 SEMAINES**

### **PHASE 4.1 : G√©n√©ration Pseudo-Labels (Semaines 15-16)**

**Objectif :** Utiliser le Teacher pour cr√©er les labels sur donn√©es r√©elles.

**Script batch inference**

 \# Pseudo-code  
teacher.eval()  
with torch.no\_grad():  
    for batch in unlabeled\_dataloader:  
        depth\_maps \= teacher(batch)  
        save\_depth\_maps(depth\_maps, batch\_ids)

1.   
2. **Calculs de temps**

   * 50,000 images √ó 0.2 sec/image \= **2.8 heures**  
   * 200,000 images √ó 0.2 sec/image \= **11 heures**  
   * Pr√©voir √ó 1.5 pour I/O et overheads  
3. **Stockage des pseudo-labels**

   * Format : numpy arrays (.npy) ou images 16-bit (.png)  
   * Compression si besoin (np.savez\_compressed)

Organisation :  
 pseudo\_labels/‚îú‚îÄ‚îÄ batch\_0000/‚îú‚îÄ‚îÄ batch\_0001/‚îî‚îÄ‚îÄ ...

*   
4. **Quality check visuel**

   * Visualiser 100 exemples al√©atoires  
   * V√©rifier coh√©rence : pas de depth maps aberrantes  
   * Plot distribution valeurs de profondeur

**Livrables :**

* ‚úÖ 50k pseudo-labels g√©n√©r√©es (minimum)  
* ‚úÖ Script inference automatis√© et test√©  
* ‚úÖ Rapport qualit√© avec exemples visuels

**Crit√®res succ√®s :**

* G√©n√©ration compl√®te en \< 24h de compute  
* Aucun fichier corrompu  
* Depth maps visuellement coh√©rentes

---

### **PHASE 4.2 : Entra√Ænement Initial (Semaines 17-20)**

**Objectif :** Premier entra√Ænement complet du Student sur 50k images.

**Configuration training**

 \# Hyperparam√®tres recommand√©s  
batch\_size \= 16  
learning\_rate \= 1e-4  
epochs \= 20  
optimizer \= AdamW(student.parameters(), lr=lr, weight\_decay=0.01)  
scheduler \= CosineAnnealingLR(optimizer, T\_max=epochs)

1.   
2. **Training loop**

   * Sauvegarder checkpoint toutes les 2 epochs  
   * Logger m√©triques : loss, learning rate, GPU memory  
   * Utiliser Weights & Biases ou TensorBoard  
3. **Monitoring**

   * Plot courbes loss (train \+ val) en temps r√©el  
   * Early stopping si val loss stagne \> 5 epochs  
   * V√©rifier overfitting : √©cart train/val loss  
4. **Debugging si convergence**

   * Si loss stagne : r√©duire LR (/10)  
   * Si explosion : gradient clipping (max\_norm=1.0)  
   * Si underfitting : augmenter epochs ou r√©duire weight decay

**Livrables :**

* ‚úÖ Mod√®le entra√Æn√© sur 50k images  
* ‚úÖ Courbes de training sauvegard√©es  
* ‚úÖ Best checkpoint s√©lectionn√©

**Crit√®res succ√®s :**

* Loss d√©croissante sur au moins 15 epochs  
* Val loss \< 0.15 (approximatif, √† ajuster)  
* Pas de crash GPU

**Plan B :**

* Si 50k trop long : r√©duire √† 20k pour proto ultra-rapide  
* Si m√©moire GPU insuffisante : batch\_size=8 \+ gradient accumulation

---

### **PHASE 4.3 : Scale-up et Optimisation (Semaines 21-24)**

**Objectif :** Entra√Æner sur dataset complet (200k+) et optimiser performances.

1. **Entra√Ænement 200k images**

   * Reprendre meilleur checkpoint de Phase 4.2  
   * Fine-tuner sur dataset √©tendu  
   * Epochs : 10-15 (d√©j√† pr√©-entra√Æn√© sur 50k)  
2. **Hyperparameter tuning**

   * Tester 2-3 learning rates (5e-5, 1e-4, 5e-4)  
   * Ajuster weight decay (0.01, 0.05)  
   * Exp√©rimenter avec data augmentation strength  
3. **Ablation studies (si temps disponible)**

   * Tester impact de $\\mathcal{L}*{gm}$ vs seulement $\\mathcal{L}*{ssi}$  
   * Comparer top-10% masking vs pas de masking  
   * Essayer diff√©rentes r√©solutions (384, 518, 640\)  
4. **Monitoring avanc√©**

   * Calculer m√©triques sur val set toutes les 2 epochs  
   * Comparer avec baseline (mod√®le officiel)  
   * Target : gap \< 20% vs mod√®le officiel sur NYU-D

**Livrables :**

* ‚úÖ Mod√®le final entra√Æn√© sur 200k+ images  
* ‚úÖ Rapport d'ablation (si fait)  
* ‚úÖ Comparaison metrics vs baseline

**Crit√®res succ√®s :**

* AbsRel \< 0.08 sur NYU-D test (vs 0.053 officiel)  
* Œ¥1 \> 0.95 (vs 0.992 officiel)  
* Temps entra√Ænement total \< 100 heures GPU

**Plan B :**

* Si 200k impossible dans temps imparti : rester √† 100k  
* Si performances d√©cevantes : analyser failure cases et documenter

---

## **PHASE 5 : √âvaluation et Analyse (Semaines 25-26)**

### **Objectifs**

Quantifier performances et comprendre limites de votre mod√®le.

### **Semaine 25 : √âvaluation quantitative**

1. **Benchmarks standards**

   * **NYU-Depth V2** (indoor) : 654 images test  
   * **KITTI** (outdoor/driving) : 697 images test  
   * Calculer toutes les m√©triques :  
     * AbsRel, RMSE, log10  
     * Œ¥1, Œ¥2, Œ¥3 (accuracy thresholds)  
2. **Comparaison multi-mod√®les**

| Mod√®le | AbsRel (NYU) | Œ¥1 (NYU) | Params |
| ----- | ----- | ----- | ----- |
| DAv2-Small (officiel) | 0.053 | 0.992 | 25M |
| Votre mod√®le (50k) | ? | ? | 25M |
| Votre mod√®le (200k) | ? | ? | 25M |

3.   
   **Analyse statistique**

   * Calculer intervalles de confiance (bootstrap)  
   * Identifier cat√©gories d'images probl√©matiques  
   * Breakdown par type de sc√®ne (indoor, outdoor, night, etc.)

### **Semaine 26 : Analyse qualitative**

1. **Visualisations**

   * Cr√©er grille comparatives : Image | Ground Truth | Votre pr√©diction | DAv2 officiel  
   * Identifier 20 best cases et 20 worst cases  
   * Analyser patterns d'√©checs  
2. **Failure mode analysis**

   * Quelles sc√®nes posent probl√®me ? (reflections, transparence, objets tr√®s fins)  
   * Erreurs li√©es aux donn√©es d'entra√Ænement ?  
   * Diff√©rences indoor vs outdoor ?  
3. **Documentation**

   * R√©diger section "R√©sultats" du rapport  
   * Cr√©er pr√©sentation avec visualisations cl√©s  
   * Documenter diff√©rence performance vs papier original

### **Livrables**

* ‚úÖ Tableau complet de m√©triques (tous benchmarks)  
* ‚úÖ Notebook analyse qualitative avec visualisations  
* ‚úÖ Section r√©sultats rapport final (10-15 pages)  
* ‚úÖ Slides pr√©sentation soutenance

### **Crit√®res de succ√®s**

* M√©triques calcul√©es sur au moins 2 benchmarks  
* Gap vs mod√®le officiel expliqu√© et document√©  
* Failure cases analys√©s en profondeur

---

## **BONUS OPTIONNEL : D√©ploiement Android (NON PRIORITAIRE)**

**‚ö†Ô∏è NE FAIRE QUE SI :**

* Phases 1-5 termin√©es avec ‚â•3 semaines d'avance  
* Au moins 1 membre de l'√©quipe a exp√©rience Android  
* Projet principal d√©j√† pr√©sentable pour soutenance

### **Si vous d√©cidez de le faire**

**Semaine 27-28 : Conversion mod√®le**

* Export PyTorch ‚Üí ONNX  
* ONNX ‚Üí TensorFlow Lite  
* Quantization INT8 avec calibration dataset

**Semaine 29-30 : Application Android**

* Setup Android Studio \+ CameraX  
* Int√©gration TFLite interpreter  
* UI basique : preview \+ depth overlay

**Crit√®re d'abandon :** Si apr√®s 1 semaine vous n'avez pas un prototype qui tourne (m√™me lent), **abandonnez** et concentrez-vous sur l'am√©lioration du mod√®le ou l'analyse.

---

## **GESTION DE PROJET ET RECOMMANDATIONS**

### **R√©partition √©quipe (si 2-3 personnes)**

**Personne 1 : Data \+ Infrastructure**

* Phases 2 et 4.1  
* Gestion datasets, dataloaders, cloud storage

**Personne 2 : Architecture \+ Training**

* Phases 3 et 4.2-4.3  
* Impl√©mentation mod√®le, optimisation entra√Ænement

**Personne 3 : √âvaluation \+ Documentation**

* Phase 5  
* Benchmarking, visualisations, r√©daction rapport

**En parall√®le (tous) :** Phases 0, 1 (lecture commune)

### **Checkpoints hebdomadaires**

**Chaque vendredi :**

* R√©union 30min : √©tat d'avancement vs planning  
* Identification blockers  
* Ajustement sprint suivant si n√©cessaire

**Livrables interm√©diaires :**

* Semaine 3 : Pr√©sentation architecture √† l'encadrant  
* Semaine 9 : Revue datasets  
* Semaine 16 : D√©mo pseudo-labels  
* Semaine 20 : Mod√®le v1.0 fonctionnel  
* Semaine 24 : R√©sultats pr√©liminaires

### **Strat√©gie de mitigation des risques**

**Risque 1 : Acc√®s GPU limit√©**

* Solution : Scripter jobs batch, lancer la nuit/weekend  
* Backup : Google Colab Pro (100h GPU/mois) ou AWS EC2 g4dn

**Risque 2 : Convergence impossible**

* D√©tection : Si apr√®s 10 epochs loss ne baisse pas  
* Solution : Revenir √† overfitting test, debugger architecture  
* Plan C : Fine-tuner mod√®le officiel (toujours acceptable)

**Risque 3 : Manque de temps Phase 4**

* D√©cision go/no-go semaine 18  
* Si retard : rester √† 50k images, approfondir analyse

### **Ressources techniques n√©cessaires**

**Hardware :**

* H100 : ‚â•50 heures compute (id√©alement 100h)  
* Stockage : 1TB SSD/NVMe (datasets \+ checkpoints)  
* RAM : 64GB recommand√©

**Software :**

* PyTorch ‚â• 2.0  
* CUDA 12.x  
* Git \+ GitHub (versioning code)  
* Weights & Biases ou TensorBoard (monitoring)

---

## **CRIT√àRES DE R√âUSSITE DU PFE**

### **Minima attendus (note ‚â• 12/20)**

* ‚úÖ Mod√®le Student impl√©ment√© et entra√Æn√©  
* ‚úÖ Entra√Ænement sur ‚â•50k images r√©ussit  
* ‚úÖ √âvaluation sur au moins 1 benchmark (NYU ou KITTI)  
* ‚úÖ Rapport complet avec m√©thodologie claire

### **Objectifs moyens (note ‚â• 14/20)**

* ‚úÖ Entra√Ænement sur 200k images  
* ‚úÖ M√©triques \< 30% du mod√®le officiel  
* ‚úÖ √âvaluation sur 2 benchmarks  
* ‚úÖ Analyse failure modes document√©e

### **Excellence (note ‚â• 16/20)**

* ‚úÖ M√©triques \< 20% du mod√®le officiel  
* ‚úÖ Ablation studies avec insights originaux  
* ‚úÖ Contributions au code open-source (PR sur repo officiel ?)  
* ‚úÖ D√©mo interactive fonctionnelle

---

## **TIMELINE R√âCAPITULATIF**

Mois 1 (S1-4)   : Phase 0-1 (Baseline \+ Th√©orie)  
Mois 2-3 (S5-12) : Phase 2 (Data Engineering)  
Mois 3-4 (S13-16): Phase 3 \+ d√©but Phase 4.1  
Mois 4-5 (S17-24): Phase 4.2-4.3 (Entra√Ænement intensif)  
Mois 6 (S25-26)  : Phase 5 (√âval \+ Rapport)

**Marge de s√©curit√© :** 2 semaines non planifi√©es pour impr√©vus, cong√©s, ou approfondissements.

---

Cette roadmap est con√ßue pour √™tre **r√©aliste, it√©rative et d√©faillible** : m√™me si vous n'atteignez pas 200k images ou les performances optimales, vous aurez un projet complet et d√©fendable. L'important est de documenter vos choix, vos obstacles, et vos apprentissages.

**Bon courage pour votre PFE \!** üöÄ

