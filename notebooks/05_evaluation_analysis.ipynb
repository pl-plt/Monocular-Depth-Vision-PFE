{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5333b721",
   "metadata": {},
   "source": [
    "# 05 — Évaluation et Analyse (Phase 5)\n",
    "\n",
    "**Objectif :** Évaluation quantitative et qualitative du modèle Student.\n",
    "\n",
    "**Analyse quantitative :**\n",
    "- Métriques sur NYU-D et KITTI\n",
    "- Comparaison avec DAv2-Small officiel\n",
    "- Intervalles de confiance (bootstrap)\n",
    "\n",
    "**Analyse qualitative :**\n",
    "- Grilles comparatives\n",
    "- 20 best + 20 worst cases\n",
    "- Failure mode analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc5f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.student import StudentModel\n",
    "from src.evaluation.benchmark import BenchmarkEvaluator\n",
    "from src.evaluation.metrics import DepthMetrics\n",
    "from src.evaluation.visualization import DepthVisualizer\n",
    "from src.utils.helpers import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d331e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/hdd/homes/ppillet/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Student] Chargement checkpoint : outputs/checkpoints/best_model.pt\n",
      "Device : NVIDIA H100 NVL MIG 1g.12gb\n",
      "  Mémoire : 11.5 GB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Charger le modèle Student entraîné\n",
    "student = StudentModel(backbone_name='dinov2_vits14')\n",
    "student.load_checkpoint('outputs/checkpoints/best_model.pt')\n",
    "student = student.to(get_device()).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b63b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation sur NYU (0 images)...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Évaluation sur benchmarks\u001b[39;00m\n\u001b[1;32m      2\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m BenchmarkEvaluator(student)\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnyu_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/benchmarks/nyu_depth_v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkitti_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/benchmarks/kitti\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/depth-anything-v2-pfe/src/evaluation/benchmark.py:200\u001b[0m, in \u001b[0;36mBenchmarkEvaluator.full_evaluation\u001b[0;34m(self, nyu_dir, kitti_dir)\u001b[0m\n\u001b[1;32m    197\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nyu_dir:\n\u001b[0;32m--> 200\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnyu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnyu_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnyu\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metrics\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompare_with_official(metrics, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnyu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/deeplearning/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/depth-anything-v2-pfe/src/evaluation/benchmark.py:128\u001b[0m, in \u001b[0;36mBenchmarkEvaluator.evaluate\u001b[0;34m(self, benchmark, data_dir, batch_size)\u001b[0m\n\u001b[1;32m    125\u001b[0m         all_ground_truths\u001b[38;5;241m.\u001b[39mappend(depth_gt[i])\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Calculer les métriques\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_ground_truths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRésultats \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(DepthMetrics\u001b[38;5;241m.\u001b[39mformat_results(metrics))\n",
      "File \u001b[0;32m~/depth-anything-v2-pfe/src/evaluation/metrics.py:138\u001b[0m, in \u001b[0;36mDepthMetrics.compute_batch\u001b[0;34m(self, predictions, ground_truths, valid_masks)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Moyenner\u001b[39;00m\n\u001b[1;32m    137\u001b[0m avg_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    139\u001b[0m     avg_metrics[key] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([m[key] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m all_metrics])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_metrics\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# TODO: Évaluation sur benchmarks\n",
    "evaluator = BenchmarkEvaluator(student)\n",
    "results = evaluator.full_evaluation(\n",
    "    nyu_dir='datasets/benchmarks/nyu_depth_v2',\n",
    "    # kitti_dir='datasets/benchmarks/kitti',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tableau comparatif\n",
    "# | Modèle               | AbsRel (NYU) | δ1 (NYU) | Params |\n",
    "# |----------------------|--------------|----------|--------|\n",
    "# | DAv2-Small (officiel)| 0.053        | 0.992    | 25M    |\n",
    "# | Notre modèle (50k)  | ?            | ?        | 25M    |\n",
    "# | Notre modèle (200k) | ?            | ?        | 25M    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8baf11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualisations best/worst cases\n",
    "# TODO: Failure mode analysis\n",
    "# TODO: Breakdown par type de scène"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
